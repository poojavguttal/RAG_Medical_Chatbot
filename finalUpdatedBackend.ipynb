{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu huggingface_hub flask flask-cors pyngrok"
      ],
      "metadata": {
        "id": "AhKzfLoQnyEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f089f1-0a23-44d1-94e2-bdcccc4c00e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ingest.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "\n",
        "def load_data(csv_path: str):\n",
        "    \"\"\"\n",
        "    Load your CSV and return two parallel lists:\n",
        "    - docs: the text to chunk (e.g. the Answer field)\n",
        "    - metas: a list of dicts with whatever metadata you want to carry along\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # adjust column names as needed\n",
        "    docs = df['Answer'].fillna('').astype(str).tolist()\n",
        "    metas = [\n",
        "        {\n",
        "            'doc_id': row['Document_ID'],\n",
        "            'question': row['Question'],\n",
        "            'source': row['Document_Source'],\n",
        "            'url': row['Document_URL']\n",
        "        }\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "    return docs, metas\n",
        "\n",
        "\n",
        "def chunk_text(text: str, size: int, overlap: int):\n",
        "    \"\"\"\n",
        "    Simple word-based chunking with overlap.\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    for start in range(0, len(tokens), size - overlap):\n",
        "        chunk = tokens[start:start + size]\n",
        "        if not chunk:\n",
        "            break\n",
        "        chunks.append(' '.join(chunk))\n",
        "        if start + size >= len(tokens):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def embed_chunks(\n",
        "    chunks: list[str],\n",
        "    tokenizer: AutoTokenizer,\n",
        "    model: AutoModel,\n",
        "    device: torch.device\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Tokenize + forward-pass each chunk, mean-pool the last hidden states.\n",
        "    Returns an (N × D) array.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_embeds = []\n",
        "    with torch.no_grad():\n",
        "        for text in chunks:\n",
        "            tokens = tokenizer(\n",
        "                text,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                padding='longest'\n",
        "            ).to(device)\n",
        "\n",
        "            outputs = model(**tokens)\n",
        "            last_hidden = outputs.last_hidden_state    # (1, seq_len, D)\n",
        "            mask = tokens['attention_mask'].unsqueeze(-1)  # (1, seq_len, 1)\n",
        "            # mean-pool only over non-padded tokens\n",
        "            summed = (last_hidden * mask).sum(dim=1)      # (1, D)\n",
        "            counts = mask.sum(dim=1)                      # (1, 1)\n",
        "            pooled = summed / counts                      # (1, D)\n",
        "            all_embeds.append(pooled.squeeze(0).cpu().numpy())\n",
        "\n",
        "    return np.vstack(all_embeds)  # shape (N, D)\n",
        "\n",
        "\n",
        "def build_and_save_index(\n",
        "    embeddings: np.ndarray,\n",
        "    metas: list[dict],\n",
        "    out_dir: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Normalize embeddings for inner-product similarity,\n",
        "    build a FlatIP index, save index + metadata.\n",
        "    \"\"\"\n",
        "    # normalize to unit length for IP = cosine\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    faiss.write_index(index, os.path.join(out_dir, 'faiss.index'))\n",
        "\n",
        "    with open(os.path.join(out_dir, 'metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metas, f)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # 1) load\n",
        "    docs, base_metas = load_data(args.csv_path)\n",
        "\n",
        "    # 2) chunk + assemble metadata\n",
        "    all_chunks = []\n",
        "    chunk_metas = []\n",
        "    for doc_text, meta in zip(docs, base_metas):\n",
        "        chunks = chunk_text(doc_text, args.chunk_size, args.overlap)\n",
        "        for i, c in enumerate(chunks):\n",
        "            all_chunks.append(c)\n",
        "            m = meta.copy()\n",
        "            m['chunk_id'] = i\n",
        "            chunk_metas.append(m)\n",
        "\n",
        "    # 3) load model + embed\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
        "    model = AutoModel.from_pretrained(args.model).to(device)\n",
        "\n",
        "    embeddings = embed_chunks(all_chunks, tokenizer, model, device)\n",
        "\n",
        "    # 4) build FAISS + save everything\n",
        "    build_and_save_index(embeddings, chunk_metas, args.out_dir)\n",
        "    print(f\"Indexed {len(all_chunks)} chunks. Index + metadata saved to '{args.out_dir}'.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"RAG ingestion: CSV → chunks → PUBMEDBERT → FAISS\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"csv_path\",\n",
        "        help=\"path to your consolidated CSV (e.g. /content/all_data.csv)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--out_dir\", \"-o\",\n",
        "        default=\"index_data\",\n",
        "        help=\"where to write faiss.index + metadata.pkl\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--chunk_size\", \"-c\", type=int, default=200,\n",
        "        help=\"max words per chunk\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overlap\", \"-l\", type=int, default=50,\n",
        "        help=\"words overlap between consecutive chunks\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model\", \"-m\",\n",
        "        default=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
        "        help=\"HuggingFace model ID for PubMedBERT\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xit85tAyo1-e",
        "outputId": "4d4f182e-8528-4242-ee75-f5d110fcecfa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ingest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ingest.py /content/all_data.csv --out_dir /content/outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5udJkYRtn6Kh",
        "outputId": "b7e2e7e9-b64b-4a63-8cc2-5663a3504065"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-16 01:30:49.297831: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747359049.331197    4606 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747359049.342170    4606 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-16 01:30:49.374870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Indexed 27193 chunks. Index + metadata saved to '/content/outputs'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile retrieve_generate.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "def load_data(csv_path: str):\n",
        "    # Load your CSV and return raw docs and base metadata\n",
        "    df = pd.read_csv(csv_path)\n",
        "    docs = df['Answer'].fillna('').astype(str).tolist()\n",
        "    metas = [\n",
        "        {\n",
        "            'doc_id': row['Document_ID'],\n",
        "            'source': row['Document_Source'],\n",
        "            'url': row['Document_URL'],\n",
        "        }\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "    return docs, metas\n",
        "\n",
        "def chunk_text(text: str, size: int, overlap: int):\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    for start in range(0, len(tokens), size - overlap):\n",
        "        chunk = tokens[start:start + size]\n",
        "        if not chunk:\n",
        "            break\n",
        "        chunks.append(' '.join(chunk))\n",
        "        if start + size >= len(tokens):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "def embed_query(text: str, tokenizer: AutoTokenizer, model: AutoModel, device: torch.device) -> np.ndarray:\n",
        "    # Embed a single query and L2-normalize (for IndexFlatIP / cosine sim)\n",
        "    model.eval()\n",
        "    toks = tokenizer(text, return_tensors='pt', truncation=True, padding='longest').to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**toks).last_hidden_state      # (1, seq_len, D)\n",
        "        mask = toks['attention_mask'].unsqueeze(-1)  # (1, seq_len, 1)\n",
        "        summed = (out * mask).sum(dim=1)            # (1, D)\n",
        "        counts = mask.sum(dim=1)                    # (1, 1)\n",
        "        pooled = (summed / counts).cpu().numpy()    # (1, D)\n",
        "    faiss.normalize_L2(pooled)\n",
        "    return pooled\n",
        "\n",
        "def get_final_response(\n",
        "    query,\n",
        "    csv_path=\"/content/all_data.csv\",\n",
        "    index_dir=\"/content/outputs\",\n",
        "    embed_model=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
        "    llm_model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    chunk_size=200,\n",
        "    overlap=50,\n",
        "    top_k=10,\n",
        "    max_new_tokens=512,\n",
        "    hf_token='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'\n",
        "):\n",
        "\n",
        "    login(token=hf_token)\n",
        "\n",
        "    # 1) Load FAISS index + metadata\n",
        "    idx = faiss.read_index(os.path.join(index_dir, \"faiss.index\"))\n",
        "    with open(os.path.join(index_dir, \"metadata.pkl\"), \"rb\") as f:\n",
        "        chunk_metas = pickle.load(f)\n",
        "\n",
        "    # 2) Reload & re-chunk docs\n",
        "    docs, _ = load_data(csv_path)\n",
        "    all_chunks = []\n",
        "    for doc_text in docs:\n",
        "        all_chunks.extend(chunk_text(doc_text, chunk_size, overlap))\n",
        "\n",
        "    # 3) Prepare embed model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embed_tokenizer = AutoTokenizer.from_pretrained(embed_model)\n",
        "    embed_model = AutoModel.from_pretrained(embed_model, device_map=\"auto\")#.to(device)\n",
        "\n",
        "    # 4) Embed query & search\n",
        "    q_vec = embed_query(query, embed_tokenizer, embed_model, device)\n",
        "    distances, indices = idx.search(q_vec, top_k)\n",
        "\n",
        "    # 5) Collect retrieved chunks\n",
        "    retrieved = []\n",
        "    for dist, idx_ in zip(distances[0], indices[0]):\n",
        "        retrieved.append({\n",
        "            \"score\": float(dist),\n",
        "            \"meta\":  chunk_metas[idx_],\n",
        "            \"text\":  all_chunks[idx_],\n",
        "        })\n",
        "\n",
        "    # 6) Build prompt\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"[{i+1}] Source: {r['meta']['source']} | Text: {r['text']}\"\n",
        "        for i, r in enumerate(retrieved)\n",
        "    )\n",
        "    system_prompt = (\n",
        "        \"You are a knowledgeable medical assistant. \"\n",
        "        \"Use the following retrieved context to answer the user's question in detailed.\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"{system_prompt}\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "\n",
        "    client = genai.Client(api_key=\"AIzaSyBfa7rvEKHMNjhWayQLUstXP27k5AP7Fz0\")\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-preview-04-17\",\n",
        "        config=types.GenerateContentConfig(\n",
        "        max_output_tokens=8000,\n",
        "    ),\n",
        "        contents=user_prompt,\n",
        "    )\n",
        "\n",
        "    answer = response.text\n",
        "\n",
        "    # 9) Display results\n",
        "    # print(\"\\n=== Retrieved Contexts ===\")\n",
        "    # for r in retrieved:\n",
        "    #     print(f\"• (score={r['score']:.4f}) {r['text'][:200]}…\\n\")\n",
        "    # print(\"\\n=== Answer ===\")\n",
        "    # print(answer)\n",
        "\n",
        "   # return answer + \"\\n\\n\" + score +  \" Context:\\n\" + context\n",
        "\n",
        "    return answer + '\\n' + '\\n'.join([f\"{r['score']:.4f} {r['text'][:2001]}.\\n\" for r in retrieved])"
      ],
      "metadata": {
        "id": "TqPvnfhEpC9T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_final_response(\"What causes Laron syndrome ?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3THO0XWxAE2",
        "outputId": "8841657f-31ab-41f1-c9e8-74edd2b9defc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, there is no information about what causes Laron syndrome. The context discusses the causes or inheritance of other conditions like Jones syndrome, Bell's palsy, blue rubber bleb nevus syndrome, Behr syndrome, and Wildervanck syndrome, but Laron syndrome is not mentioned.\n",
            "0.9657 What causes Jones syndrome? The exact, underlying genetic cause of Jones syndrome is not yet known..\n",
            "\n",
            "0.9640 Is Williams syndrome inherited?.\n",
            "\n",
            "0.9625 What causes Bell's palsy?.\n",
            "\n",
            "0.9616 What causes blue rubber bleb nevus syndrome? Currently the cause of blue rubber bleb syndrome is not known..\n",
            "\n",
            "0.9611 How is oculopharyngeal muscular dystrophy inherited?.\n",
            "\n",
            "0.9600 Is genetic testing available for occipital horn syndrome?.\n",
            "\n",
            "0.9590 What causes Behr syndrome? The exact cause of Behr syndrome is not known; however, a genetic cause is suspected based on the families identified, thus far..\n",
            "\n",
            "0.9585 Researchers are not sure how common Shwachman-Diamond syndrome is. Several hundred cases have been reported in scientific studies..\n",
            "\n",
            "0.9578 What causes Wildervanck syndrome? The exact cause of Wildervanck syndrome is not known. It is suspected to be a polygenic condition, meaning that many genetic factors may be involved..\n",
            "\n",
            "0.9572 Crouzonodermoskeletal syndrome is rare; this condition is seen in about 1 person per million..\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "5NRKerEYAiCs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "\n",
        "# =================== SET YOUR TOKENS ===================\n",
        "conf.get_default().auth_token = \"2x9QwOR9RwW6iKUQDqkPeL54lza_2AnJ8kVPZLKEAvEaVQupk\"\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# =================== SETUP FLASK ===================\n",
        "app = Flask(__name__)\n",
        "CORS(app, resources={r\"/api/*\": {\"origins\": \"*\"}}, supports_credentials=True)\n",
        "\n",
        "@app.route(\"/api/ask\", methods=[\"GET\", \"POST\"])\n",
        "def ask():\n",
        "    if request.method == \"GET\":\n",
        "        return \"✅ Flask is running and /api/ask is reachable. Use POST to query.\"\n",
        "\n",
        "    query = request.json.get(\"query\")\n",
        "    print(\"📥 Received query:\", query)\n",
        "\n",
        "    try:\n",
        "        answer = get_final_response(query, hf_token=HF_TOKEN)\n",
        "        return jsonify({\"answer\": answer})\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error:\", str(e))\n",
        "        return jsonify({\"answer\": f\"Error: {str(e)}\"}), 500\n",
        "\n",
        "# =================== START SERVER ===================\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"\\n🌐 Public URL (paste into frontend):\", public_url)\n",
        "\n",
        "# Run Flask in a thread so it doesn't block\n",
        "threading.Thread(target=app.run, kwargs={\"port\": 5000}).start()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HOJyJL0AZ83U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc4a2bb-657c-4086-f813-d0be364c3355"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🌐 Public URL (paste into frontend): NgrokTunnel: \"https://3f06-34-125-67-162.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "r = requests.post(\"http://127.0.0.1:5000/api/ask\", json={\"query\": \"What causes L-arginine:glycine amidinotransferase deficiency?\"})\n",
        "print(r.json())"
      ],
      "metadata": {
        "id": "wiial6OlwUHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdcfe2b-20e7-427b-f59c-ea4ca37c9101"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Received query: What causes L-arginine:glycine amidinotransferase deficiency?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "INFO:werkzeug:127.0.0.1 - - [16/May/2025 02:35:05] \"POST /api/ask HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': \"Based on the provided context:\\n\\nL-arginine:glycine amidinotransferase deficiency is caused by mutations in the **GATM gene**.\\n\\nHere's a detailed explanation:\\n1.  The **GATM gene** provides instructions for making the enzyme called **arginine:glycine amidinotransferase**.\\n2.  This enzyme plays a crucial role in the first step of the two-step process that produces **creatine** from the amino acids glycine, arginine, and methionine.\\n3.  Specifically, the enzyme transfers a guanidino group from arginine to glycine, producing guanidinoacetic acid (which is then converted to creatine).\\n4.  Creatine is essential for the body to store and use energy properly.\\n5.  **Mutations in the GATM gene** lead to a deficiency of the arginine:glycine amidinotransferase enzyme or impair its ability to function correctly in creatine synthesis.\\n6.  This impairment results in a **shortage of creatine** in the body.\\n7.  The effects of this creatine shortage are most severe in organs and tissues that require large amounts of energy, particularly the brain.\\n0.9622 Mutations in the GATM gene cause arginine:glycine amidinotransferase deficiency. The GATM gene provides instructions for making the enzyme arginine:glycine amidinotransferase. This enzyme participates in the two-step production (synthesis) of the compound creatine from the protein building blocks (amino acids) glycine, arginine, and methionine. Specifically, arginine:glycine amidinotransferase controls the first step of the process. In this step, a compound called guanidinoacetic acid is produced by transferring a cluster of nitrogen and hydrogen atoms called a guanidino group from arginine to glycine. Guanidinoacetic acid is converted to creatine in the second step of the process. Creatine is needed for the body to store and use energy properly. GATM gene mutations impair the ability of the arginine:glycine amidinotransferase enzyme to participate in creatine synthesis, resulting in a shortage of creatine. The effects of arginine:glycine amidinotransferase deficiency are most severe in organs and tissues that require large amounts of energy, especially the brain..\\n\\n0.9591 Mutations in the OTC gene cause ornithine transcarbamylase deficiency. Ornithine transcarbamylase deficiency belongs to a class of genetic diseases called urea cycle disorders. The urea cycle is a sequence of reactions that occurs in liver cells. It processes excess nitrogen, generated when protein is used by the body, to make a compound called urea that is excreted by the kidneys. In ornithine transcarbamylase deficiency, the enzyme that starts a specific reaction within the urea cycle is damaged or missing. The urea cycle cannot proceed normally, and nitrogen accumulates in the bloodstream in the form of ammonia. Ammonia is especially damaging to the nervous system, so ornithine transcarbamylase deficiency causes neurological problems as well as eventual damage to the liver..\\n\\n0.9586 What causes 3-hydroxyisobutyric aciduria? In many affected people, the exact underlying cause of 3-hydroxyisobutyric aciduria is poorly understood. Scientists believe that some cases are caused by changes (mutations) in the ALDH6A1 gene. This gene encodes an enzyme called methylmalonate semialdehyde dehydrogenase, which helps the body break down certain amino acids (the building blocks of protein) found in food. If this gene isn't working properly, the body is unable to break down the amino acids valine and thymine which leads to a build-up of toxic substances in the body and the many signs and symptoms of 3-hydroxyisobutyric aciduria..\\n\\n0.9584 What causes ornithine transcarbamylase (OTC) deficiency? Ornithine transcarbamylase (OTC) deficiency is caused by mutations in the OTC gene. OTC deficiency belongs to a class of genetic diseases called urea cycle disorders. The urea cycle is a sequence of reactions that occurs in liver cells. It processes excess nitrogen, generated when protein is used by the body, to make a compound called urea that is excreted by the kidneys. In OTC deficiency, the enzyme that starts a specific reaction within the urea cycle is damaged or missing. The urea cycle cannot proceed normally, and nitrogen accumulates in the bloodstream in the form of ammonia. Ammonia is especially damaging to the nervous system, so OTC deficiency causes neurological problems as well as eventual damage to the liver..\\n\\n0.9578 gene provides instructions for producing the enzyme S-adenosylhomocysteine hydrolase. This enzyme converts the AdoHcy into the compound homocysteine. Homocysteine may be converted back to methionine or into another amino acid, cysteine. A deficiency of any of these enzymes results in a buildup of methionine in the body, and may cause signs and symptoms related to hypermethioninemia..\\n\\n0.9575 Mutations in the ASL gene cause argininosuccinic aciduria. Argininosuccinic aciduria belongs to a class of genetic diseases called urea cycle disorders. The urea cycle is a sequence of reactions that occur in liver cells. It processes excess nitrogen, generated when protein is used by the body, to make a compound called urea that is excreted by the kidneys. In argininosuccinic aciduria, the enzyme that starts a specific reaction within the urea cycle is damaged or missing. The urea cycle cannot proceed normally, and nitrogen accumulates in the bloodstream in the form of ammonia. Ammonia is especially damaging to the nervous system, so argininosuccinic aciduria causes neurological problems as well as eventual damage to the liver..\\n\\n0.9575 Mutations in the NAGS gene cause N-acetylglutamate synthase deficiency. N-acetylglutamate synthase deficiency belongs to a class of genetic diseases called urea cycle disorders. The urea cycle is a sequence of reactions that occurs in liver cells. This cycle processes excess nitrogen, generated when protein is used by the body, to make a compound called urea that is excreted by the kidneys. The NAGS gene provides instructions for making the enzyme N-acetylglutamate synthase, which helps produce a compound called N-acetylglutamate. This compound is needed to activate another enzyme, carbamoyl phosphate synthetase I, which controls the first step of the urea cycle. In people with N-acetylglutamate synthase deficiency, N-acetylglutamate is not available in sufficient quantities, or is not present at all. As a result, urea cannot be produced normally, and excess nitrogen accumulates in the blood in the form of ammonia. This accumulation of ammonia causes the neurological problems and other signs and symptoms of N-acetylglutamate synthase deficiency..\\n\\n0.9570 What causes succinic semialdehyde dehydrogenase deficiency? Succinic semialdehyde dehydrogenase deficiency (SSADH) is caused by mutations in the ALDH5A1 gene. This gene provides instructions for producing the succinic semialdehyde dehydrogenase enzyme which is involved in the breakdown of a chemical that transmits signals in the brain (neurotransmitter) called gamma-amino butyric acid (GABA). The primary role of GABA is to prevent the brain from being overloaded with too many signals. A shortage (deficiency) of succinic semialdehyde dehydrogenase leads to an increase in the amount of GABA and a related molecule called gamma-hydroxybutyrate (GHB) in the body, particularly the brain and spinal cord (central nervous system). It is unclear how an increase in GABA and GHB causes developmental delay, seizures, and other signs and symptoms of succinic semialdehyde dehydrogenase deficiency..\\n\\n0.9569 What causes primary hyperoxaluria type 2? Researchers have identified more than a dozen GRHPR mutations that cause this condition. These mutations either introduce signals that disrupt production of the glyoxylate reductase/hydroxypyruvate reductase enzyme or alter its structure. As a result, enzyme activity is absent or dramatically reduced. Glyoxylate builds up because of the enzyme shortage, and is converted to a compound called oxalate instead of glycolate. Oxalate, in turn, combines with calcium to form calcium oxalate, which the body cannot readily eliminate. Deposits of calcium oxalate can lead to the characteristic features of primary hyperoxaluria type 2..\\n\\n0.9567 Mutations in the CPS1 gene cause carbamoyl phosphate synthetase I deficiency. The CPS1 gene provides instructions for making the enzyme carbamoyl phosphate synthetase I. This enzyme participates in the urea cycle, which is a sequence of biochemical reactions that occurs in liver cells. The urea cycle processes excess nitrogen, generated when protein is broken down by the body, to make a compound called urea that is excreted by the kidneys. The specific role of the carbamoyl phosphate synthetase I enzyme is to control the first step of the urea cycle, a reaction in which excess nitrogen compounds are incorporated into the cycle to be processed. Carbamoyl phosphate synthetase I deficiency belongs to a class of genetic diseases called urea cycle disorders. In this condition, the carbamoyl phosphate synthetase I enzyme is at low levels (deficient) or absent, and the urea cycle cannot proceed normally. As a result, nitrogen accumulates in the bloodstream in the form of toxic ammonia instead of being converted to less toxic urea and excreted. Ammonia is especially damaging to the brain, and excess ammonia causes neurological problems and other signs and symptoms of carbamoyl phosphate synthetase I deficiency..\\n\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZA5CQz80sJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}