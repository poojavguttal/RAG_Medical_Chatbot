{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5BLRt3TVxLl",
        "outputId": "0d78c2d5-d8d8-4bad-e0e2-78cdb4db2d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting flask-cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok, flask-cors\n",
            "Successfully installed flask-cors-5.0.1 pyngrok-7.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhKzfLoQnyEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aac1969-5bff-4f58-ddee-8e6fff0c9a1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qq install faiss-cpu huggingface_hub tiktoken blobfile bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xit85tAyo1-e",
        "outputId": "f257d8de-9ac5-4775-bb7f-61d767b7d73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ingest.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ingest.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "\n",
        "def load_data(csv_path: str):\n",
        "    \"\"\"\n",
        "    Load your CSV and return two parallel lists:\n",
        "    - docs: the text to chunk (e.g. the Answer field)\n",
        "    - metas: a list of dicts with whatever metadata you want to carry along\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # adjust column names as needed\n",
        "    docs = df['Answer'].fillna('').astype(str).tolist()\n",
        "    metas = [\n",
        "        {\n",
        "            'doc_id': row['Document_ID'],\n",
        "            'question': row['Question'],\n",
        "            'source': row['Document_Source'],\n",
        "            'url': row['Document_URL']\n",
        "        }\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "    return docs, metas\n",
        "\n",
        "\n",
        "def chunk_text(text: str, size: int, overlap: int):\n",
        "    \"\"\"\n",
        "    Simple word-based chunking with overlap.\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    for start in range(0, len(tokens), size - overlap):\n",
        "        chunk = tokens[start:start + size]\n",
        "        if not chunk:\n",
        "            break\n",
        "        chunks.append(' '.join(chunk))\n",
        "        if start + size >= len(tokens):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def embed_chunks(\n",
        "    chunks: list[str],\n",
        "    tokenizer: AutoTokenizer,\n",
        "    model: AutoModel,\n",
        "    device: torch.device\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Tokenize + forward-pass each chunk, mean-pool the last hidden states.\n",
        "    Returns an (N √ó D) array.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_embeds = []\n",
        "    with torch.no_grad():\n",
        "        for text in chunks:\n",
        "            tokens = tokenizer(\n",
        "                text,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                padding='longest'\n",
        "            ).to(device)\n",
        "\n",
        "            outputs = model(**tokens)\n",
        "            last_hidden = outputs.last_hidden_state    # (1, seq_len, D)\n",
        "            mask = tokens['attention_mask'].unsqueeze(-1)  # (1, seq_len, 1)\n",
        "            # mean-pool only over non-padded tokens\n",
        "            summed = (last_hidden * mask).sum(dim=1)      # (1, D)\n",
        "            counts = mask.sum(dim=1)                      # (1, 1)\n",
        "            pooled = summed / counts                      # (1, D)\n",
        "            all_embeds.append(pooled.squeeze(0).cpu().numpy())\n",
        "\n",
        "    return np.vstack(all_embeds)  # shape (N, D)\n",
        "\n",
        "\n",
        "def build_and_save_index(\n",
        "    embeddings: np.ndarray,\n",
        "    metas: list[dict],\n",
        "    out_dir: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Normalize embeddings for inner-product similarity,\n",
        "    build a FlatIP index, save index + metadata.\n",
        "    \"\"\"\n",
        "    # normalize to unit length for IP = cosine\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    faiss.write_index(index, os.path.join(out_dir, 'faiss.index'))\n",
        "\n",
        "    with open(os.path.join(out_dir, 'metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metas, f)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # 1) load\n",
        "    docs, base_metas = load_data(args.csv_path)\n",
        "\n",
        "    # 2) chunk + assemble metadata\n",
        "    all_chunks = []\n",
        "    chunk_metas = []\n",
        "    for doc_text, meta in zip(docs, base_metas):\n",
        "        chunks = chunk_text(doc_text, args.chunk_size, args.overlap)\n",
        "        for i, c in enumerate(chunks):\n",
        "            all_chunks.append(c)\n",
        "            m = meta.copy()\n",
        "            m['chunk_id'] = i\n",
        "            chunk_metas.append(m)\n",
        "\n",
        "    # 3) load model + embed\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
        "    model = AutoModel.from_pretrained(args.model).to(device)\n",
        "\n",
        "    embeddings = embed_chunks(all_chunks, tokenizer, model, device)\n",
        "\n",
        "    # 4) build FAISS + save everything\n",
        "    build_and_save_index(embeddings, chunk_metas, args.out_dir)\n",
        "    print(f\"Indexed {len(all_chunks)} chunks. Index + metadata saved to '{args.out_dir}'.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"RAG ingestion: CSV ‚Üí chunks ‚Üí PUBMEDBERT ‚Üí FAISS\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"csv_path\",\n",
        "        help=\"path to your consolidated CSV (e.g. all_data.csv)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--out_dir\", \"-o\",\n",
        "        default=\"index_data\",\n",
        "        help=\"where to write faiss.index + metadata.pkl\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--chunk_size\", \"-c\", type=int, default=200,\n",
        "        help=\"max words per chunk\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overlap\", \"-l\", type=int, default=50,\n",
        "        help=\"words overlap between consecutive chunks\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model\", \"-m\",\n",
        "        default=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
        "        help=\"HuggingFace model ID for PubMedBERT\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5udJkYRtn6Kh",
        "outputId": "ca696a1b-36ae-4a8f-9e28-32710bb06dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 229kB/s]\n",
            "config.json: 100% 385/385 [00:00<00:00, 2.66MB/s]\n",
            "vocab.txt: 100% 225k/225k [00:00<00:00, 3.49MB/s]\n",
            "2025-05-14 04:32:55.104899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747197175.390533    1935 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747197175.466718    1935 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-14 04:32:56.087511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "pytorch_model.bin: 100% 440M/440M [00:01<00:00, 248MB/s]\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Indexed 27193 chunks. Index + metadata saved to '/outputs'.\n"
          ]
        }
      ],
      "source": [
        "!python ingest.py all_data.csv --out_dir /outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "TqPvnfhEpC9T",
        "outputId": "eff2a265-dbe6-45a9-d3b6-b3b09e9fe41b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: RAG Retrieval + Llama-2 Generation [-h] --csv_path CSV_PATH\n",
            "                                          [--index_dir INDEX_DIR]\n",
            "                                          [--embed_model EMBED_MODEL]\n",
            "                                          [--llm_model LLM_MODEL]\n",
            "                                          [--chunk_size CHUNK_SIZE]\n",
            "                                          [--overlap OVERLAP] [--top_k TOP_K]\n",
            "                                          --query QUERY\n",
            "                                          [--max_new_tokens MAX_NEW_TOKENS]\n",
            "                                          [--hf_token HF_TOKEN]\n",
            "RAG Retrieval + Llama-2 Generation: error: the following arguments are required: --csv_path/-c, --query/-q\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "%%writefile retrieve_generate.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "def load_data(csv_path: str):\n",
        "    # Load your CSV and return raw docs and base metadata\n",
        "    df = pd.read_csv(csv_path)\n",
        "    docs = df['Answer'].fillna('').astype(str).tolist()\n",
        "    metas = [\n",
        "        {\n",
        "            'doc_id': row['Document_ID'],\n",
        "            'source': row['Document_Source'],\n",
        "            'url': row['Document_URL'],\n",
        "        }\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "    return docs, metas\n",
        "\n",
        "def chunk_text(text: str, size: int, overlap: int):\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    for start in range(0, len(tokens), size - overlap):\n",
        "        chunk = tokens[start:start + size]\n",
        "        if not chunk:\n",
        "            break\n",
        "        chunks.append(' '.join(chunk))\n",
        "        if start + size >= len(tokens):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "def embed_query(text: str, tokenizer: AutoTokenizer, model: AutoModel, device: torch.device) -> np.ndarray:\n",
        "    # Embed a single query and L2-normalize (for IndexFlatIP / cosine sim)\n",
        "    model.eval()\n",
        "    toks = tokenizer(text, return_tensors='pt', truncation=True, padding='longest').to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**toks).last_hidden_state      # (1, seq_len, D)\n",
        "        mask = toks['attention_mask'].unsqueeze(-1)  # (1, seq_len, 1)\n",
        "        summed = (out * mask).sum(dim=1)            # (1, D)\n",
        "        counts = mask.sum(dim=1)                    # (1, 1)\n",
        "        pooled = (summed / counts).cpu().numpy()    # (1, D)\n",
        "    faiss.normalize_L2(pooled)\n",
        "    return pooled\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\"RAG Retrieval + Llama-2 Generation\")\n",
        "    parser.add_argument(\"--csv_path\",    \"-c\", required=True, help=\"Your consolidated CSV\")\n",
        "    parser.add_argument(\"--index_dir\",   \"-i\", default=\"index_data\", help=\"Where ingest.py saved faiss.index + metadata.pkl\")\n",
        "    parser.add_argument(\"--embed_model\", \"-e\", default=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
        "                        help=\"HuggingFace PubMedBERT model for embeddings\")\n",
        "    parser.add_argument(\"--llm_model\",   \"-l\", default=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                        help=\"HuggingFace Llama-2 chat model\")\n",
        "    parser.add_argument(\"--chunk_size\",  type=int, default=200, help=\"Must match ingest.py\")\n",
        "    parser.add_argument(\"--overlap\",     type=int, default=50,  help=\"Must match ingest.py\")\n",
        "    parser.add_argument(\"--top_k\",       type=int, default=5,   help=\"How many chunks to retrieve\")\n",
        "    parser.add_argument(\"--query\",       \"-q\", required=True,   help=\"Your question\")\n",
        "    parser.add_argument(\"--max_new_tokens\", type=int, default=512, help=\"Generation length\")\n",
        "    parser.add_argument(\"--hf_token\", help=\"hugging face token error\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    login(token=args.hf_token)\n",
        "\n",
        "    # 1) Load FAISS index + metadata\n",
        "    idx = faiss.read_index(os.path.join(args.index_dir, \"faiss.index\"))\n",
        "    with open(os.path.join(args.index_dir, \"metadata.pkl\"), \"rb\") as f:\n",
        "        chunk_metas = pickle.load(f)\n",
        "\n",
        "    # 2) Re-load & re-chunk docs to recover chunk texts (same order as ingest.py)\n",
        "    docs, base_metas = load_data(args.csv_path)\n",
        "    all_chunks = []\n",
        "    for doc_text in docs:\n",
        "        all_chunks.extend(chunk_text(doc_text, args.chunk_size, args.overlap))\n",
        "\n",
        "    # 3) Prepare embed model for retrieval\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embed_tokenizer = AutoTokenizer.from_pretrained(args.embed_model)\n",
        "    embed_model     = AutoModel.from_pretrained(args.embed_model).to(device)\n",
        "\n",
        "    # 4) Embed the query and search\n",
        "    q_vec = embed_query(args.query, embed_tokenizer, embed_model, device)\n",
        "    distances, indices = idx.search(q_vec, args.top_k)\n",
        "\n",
        "    # 5) Gather retrieved context\n",
        "    retrieved = []\n",
        "    for dist, idx_ in zip(distances[0], indices[0]):\n",
        "        retrieved.append({\n",
        "            \"score\": float(dist),\n",
        "            \"meta\":  chunk_metas[idx_],\n",
        "            \"text\":  all_chunks[idx_],\n",
        "        })\n",
        "\n",
        "    # 6) Build prompt for Llama-2 chat\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"[{i+1}] Source: {r['meta']['source']} | Text: {r['text']}\"\n",
        "        for i, r in enumerate(retrieved)\n",
        "    )\n",
        "    system_prompt = (\n",
        "        \"You are a knowledgeable medical assistant. \"\n",
        "        \"Use the following retrieved context to answer the user's question.\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"{system_prompt}\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"Question: {args.query}\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    # 7) Load Llama-2 chat model (4-bit quantized) for generation\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "    llm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "        args.llm_model, use_fast=True, use_auth_token=True\n",
        "    )\n",
        "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.llm_model,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "    )\n",
        "\n",
        "    # 8) Tokenize & generate\n",
        "    inputs = llm_tokenizer(user_prompt, return_tensors=\"pt\").to(llm_model.device)\n",
        "    output_ids = llm_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=args.max_new_tokens,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        eos_token_id=llm_tokenizer.eos_token_id,\n",
        "        pad_token_id=llm_tokenizer.pad_token_id,\n",
        "    )\n",
        "    answer = llm_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # 9) Print out the answer + options for inspection\n",
        "    print(\"\\n=== Retrieved Contexts ===\")\n",
        "    for r in retrieved:\n",
        "        print(f\"‚Ä¢ (score={r['score']:.4f}) {r['text'][:200]}‚Ä¶\\n\")\n",
        "    print(\"\\n=== Answer ===\")\n",
        "    print(answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8hRXGxiYJ7m"
      },
      "outputs": [],
      "source": [
        "def get_final_response(\n",
        "    query,\n",
        "    csv_path=\"all_data.csv\",\n",
        "    index_dir=\"/outputs\",\n",
        "    embed_model=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
        "    llm_model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    chunk_size=200,\n",
        "    overlap=50,\n",
        "    top_k=5,\n",
        "    max_new_tokens=512,\n",
        "    hf_token=None\n",
        "):\n",
        "    import os, pickle, faiss, torch\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "    from huggingface_hub import login\n",
        "\n",
        "    def load_data(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        docs = df['Answer'].fillna('').astype(str).tolist()\n",
        "        metas = [\n",
        "            {'doc_id': row['Document_ID'], 'source': row['Document_Source'], 'url': row['Document_URL']}\n",
        "            for _, row in df.iterrows()\n",
        "        ]\n",
        "        return docs, metas\n",
        "\n",
        "    def chunk_text(text, size, overlap):\n",
        "        tokens = text.split()\n",
        "        return [' '.join(tokens[i:i+size]) for i in range(0, len(tokens), size - overlap)]\n",
        "\n",
        "    def embed_query(text, tokenizer, model, device):\n",
        "        toks = tokenizer(text, return_tensors='pt', truncation=True, padding='longest').to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(**toks).last_hidden_state\n",
        "            mask = toks['attention_mask'].unsqueeze(-1)\n",
        "            pooled = ((out * mask).sum(dim=1) / mask.sum(dim=1)).cpu().numpy()\n",
        "        faiss.normalize_L2(pooled)\n",
        "        return pooled\n",
        "\n",
        "    # ‚úÖ Authenticate\n",
        "    login(token=hf_token)\n",
        "\n",
        "    # Load index + data\n",
        "    idx = faiss.read_index(os.path.join(index_dir, \"faiss.index\"))\n",
        "    with open(os.path.join(index_dir, \"metadata.pkl\"), \"rb\") as f:\n",
        "        chunk_metas = pickle.load(f)\n",
        "\n",
        "    docs, base_metas = load_data(csv_path)\n",
        "    all_chunks = [chunk for doc in docs for chunk in chunk_text(doc, chunk_size, overlap)]\n",
        "\n",
        "    # Embed\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embed_tokenizer = AutoTokenizer.from_pretrained(embed_model)\n",
        "    embed_model = AutoModel.from_pretrained(embed_model).to(device)\n",
        "    q_vec = embed_query(query, embed_tokenizer, embed_model, device)\n",
        "    distances, indices = idx.search(q_vec, top_k)\n",
        "\n",
        "    # Prompt\n",
        "    retrieved = [\n",
        "        {\"score\": float(dist), \"meta\": chunk_metas[idx_], \"text\": all_chunks[idx_]}\n",
        "        for dist, idx_ in zip(distances[0], indices[0])\n",
        "    ]\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"[{i+1}] Source: {r['meta']['source']} | Text: {r['text']}\" for i, r in enumerate(retrieved)\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"You are a knowledgeable medical assistant.\\n\\nContext:\\n{context}\\n\\n\"\n",
        "        f\"Question: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model, use_fast=True, use_auth_token=True)\n",
        "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        llm_model,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "    )\n",
        "    inputs = llm_tokenizer(user_prompt, return_tensors=\"pt\").to(llm_model.device)\n",
        "    output_ids = llm_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        eos_token_id=llm_tokenizer.eos_token_id,\n",
        "        pad_token_id=llm_tokenizer.pad_token_id,\n",
        "    )\n",
        "    return llm_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NRKerEYAiCs"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel\n",
        "!lt --port 5000 --subdomain youruniquename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4J-wFe6SRff",
        "outputId": "c71791bb-19cc-4022-d37f-d29a3c09ce01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0Kyour url is: https://youruniquename.loca.lt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 21:41:07] \"OPTIONS /api/ask HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Received query: what is cure for cough?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 21:41:07] \"\u001b[35m\u001b[1mPOST /api/ask HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Error: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open /content/outputs/faiss.index for reading: No such file or directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 21:44:55] \"OPTIONS /api/ask HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOJyJL0AZ83U",
        "outputId": "00cc51d4-84bd-4c34-90f8-2165d1ecceea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üåê Ngrok URL (use this in your React frontend): NgrokTunnel: \"https://e5e0-34-34-110-22.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# updated_backend_api.py\n",
        "\n",
        "import threading\n",
        "from flask import Flask, request, jsonify, make_response\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "\n",
        "conf.get_default().auth_token = \"2wvOhQm5BR5UeErvyNiT9Q1gKMJ_5WQbX4J8Vkt6uab9UNmxD\"\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "CORS(\n",
        "    app,\n",
        "    resources={r\"/*\": {\"origins\": \"*\"}},\n",
        "    supports_credentials=True\n",
        ")\n",
        "\n",
        "@app.before_request\n",
        "def handle_options():\n",
        "    if request.method == \"OPTIONS\":\n",
        "        resp = make_response()\n",
        "        resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n",
        "        resp.headers[\"Access-Control-Allow-Methods\"] = \"GET,POST,OPTIONS\"\n",
        "        resp.headers[\"Access-Control-Allow-Headers\"] = \"Content-Type,Authorization\"\n",
        "        return resp\n",
        "\n",
        "@app.route(\"/api/ask\", methods=[\"GET\", \"POST\", \"OPTIONS\"])\n",
        "def ask():\n",
        "    if request.method == \"GET\":\n",
        "        return \"‚úÖ Flask is running! POST JSON {query: ...} to get an answer.\"\n",
        "\n",
        "    data = request.get_json(force=True)\n",
        "    query = data.get(\"query\", \"\")\n",
        "    print(\"üì• Received query:\", query)\n",
        "\n",
        "    try:\n",
        "        answer = get_final_response(query, hf_token=HF_TOKEN)\n",
        "        return jsonify({\"answer\": answer})\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error in get_final_response:\", e)\n",
        "        return jsonify({\"answer\": f\"Error: {e}\"}), 500\n",
        "\n",
        "def run_app():\n",
        "    app.run(host=\"0.0.0.0\", port=5000, debug=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ngrok.kill()\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"\\nüåê Ngrok URL (use this in your React frontend): {public_url}\\n\")\n",
        "    threading.Thread(target=run_app, daemon=True).start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiial6OlwUHP",
        "outputId": "7ff832e9-e873-44c9-c449-ce26a93a9abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Received query: What is anemia?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 18:40:09] \"\u001b[35m\u001b[1mPOST /api/ask HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Error: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
            "{'answer': 'Error: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. '}\n"
          ]
        }
      ],
      "source": [
        "# Testing url\n",
        "\n",
        "import requests\n",
        "r = requests.post(\"http://127.0.0.1:5000/api/ask\", json={\"query\": \"What is anemia?\"})\n",
        "print(r.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xHPWyXBo7FE"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaZQKm7OuXAh"
      },
      "outputs": [],
      "source": [
        "!python retrieve_generate.py --csv_path /content/all_data.csv --index_dir=/content/outputs --query \"What causes L-arginine:glycine amidinotransferase deficiency?\" --hf_token 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"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "!pip install sacrebleu nltk --quiet\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "from sacrebleu import corpus_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_bleu(csv_path: str, hf_token: str):\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    questions = df['Question'].tolist()\n",
        "    references = df['Answer'].tolist()\n",
        "\n",
        "    preds = []\n",
        "    for q in questions:\n",
        "        try:\n",
        "            ans = get_final_response(q, hf_token=hf_token)\n",
        "        except ValueError as e:\n",
        "            msg = str(e)\n",
        "            if \"Some modules are dispatched on the CPU or the disk\" in msg:\n",
        "\n",
        "                ans = \"\"\n",
        "            else:\n",
        "\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            print(f\" Unexpected error on query {q!r}: {e}\")\n",
        "            ans = \"\"\n",
        "        preds.append(ans)\n",
        "\n",
        "    bleu = corpus_bleu(preds, [references])\n",
        "    print(f\"\\n‚û°Ô∏è Corpus BLEU: {bleu.score:.2f}\\n\")\n",
        "    print(bleu)\n",
        "\n",
        "    print(\"\\n‚û°Ô∏è Sentence-level BLEU:\")\n",
        "    smooth = SmoothingFunction().method4\n",
        "    for i, (p, r) in enumerate(zip(preds, references), 1):\n",
        "        score = sentence_bleu([r.split()], p.split(), smoothing_function=smooth)\n",
        "        print(f\"{i:3d}: {score:.3f}\")\n",
        "\n",
        "evaluate_bleu(\n",
        "    csv_path=\"all_data.csv\",\n",
        "    hf_token=\"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\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "xeVkgR0a86kI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}